# ğŸš€ File2RAG

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£, giÃºp chuyá»ƒn Ä‘á»•i tÃ i liá»‡u thÃ nh cÆ¡ sá»Ÿ tri thá»©c cÃ³ thá»ƒ tÃ¬m kiáº¿m báº±ng cÃ´ng nghá»‡ vector embedding vÃ  Milvus vector database.

## âš¡ Quick Start

```bash
# 1. Clone vÃ  cÃ i Ä‘áº·t
git clone https://github.com/yourusername/file2rag.git
cd file2rag
pip install -r requirements.txt

# 2. Táº¡o .env file vá»›i Gemini API key
echo "GEMINI_API_KEY=your_gemini_api_key_here" > .env

# 3. Khá»Ÿi Ä‘á»™ng Milvus
docker run -d --name milvus-standalone -p 19530:19530 milvusdb/milvus:v2.3.3

# 4. Xá»­ lÃ½ document Ä‘áº§u tiÃªn
python -c "
from rag.pipeline.rag_pipeline import RAGPipeline
rag = RAGPipeline(collection_name='demo')
doc_id = rag.process_document('your_document.pdf')
print(f'âœ… Processed: {doc_id}')
"
```

## ğŸ“– Demo nhanh

### Xá»­ lÃ½ file PDF:
```python
from rag.pipeline.rag_pipeline import RAGPipeline

# Khá»Ÿi táº¡o vÃ  xá»­ lÃ½ document
rag = RAGPipeline(collection_name="my_docs")
doc_id = rag.process_document("report.pdf")

# Output:
# ğŸš€ Initializing RAG Pipeline...
# âœ… RAG Pipeline initialized successfully!
# ğŸ“„ Processing document: report.pdf
# ğŸ“‚ Loading document... (2.1s)
# âœ‚ï¸ Chunking documents... (0.3s)
# ğŸ§  Creating embeddings... (8.7s)
# ğŸ’¾ Storing in vector database... (1.2s)
# âœ… Document processed successfully in 12.34s
```

### Batch processing:
```python
files = ["doc1.pdf", "data.csv", "notes.txt", "sheet.xlsx"]
for file in files:
    doc_id = rag.process_document(file)
    print(f"âœ… {file} â†’ {doc_id}")
```

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- **ğŸ“„ Há»— trá»£ Ä‘a Ä‘á»‹nh dáº¡ng**: PDF, DOCX, TXT, CSV, XLSX vÃ  URL web
- **ğŸ§  Text Chunking thÃ´ng minh**: Tá»± Ä‘á»™ng chá»n chiáº¿n lÆ°á»£c phÃ¹ há»£p cho tá»«ng loáº¡i tÃ i liá»‡u
- **ğŸ” TÃ¬m kiáº¿m vector**: Powered by Google Gemini text-embedding-004 (768 dimensions)
- **ğŸ’¾ Milvus Vector Database**: LÆ°u trá»¯ vÃ  tÃ¬m kiáº¿m COSINE similarity hiá»‡u suáº¥t cao
- **âš¡ Pipeline tá»± Ä‘á»™ng**: Load â†’ Chunk â†’ Embed â†’ Store trong má»™t lá»‡nh
- **ğŸ”§ Chunking linh hoáº¡t**: Cáº¥u hÃ¬nh chunk size cho text vÃ  table data
- **ğŸ§¹ Text preprocessing**: Tá»± Ä‘á»™ng lÃ m sáº¡ch excessive whitespace

## ğŸ—ï¸ Kiáº¿n trÃºc Ä‘Æ¡n giáº£n

```
ğŸ“„ Documents â†’ ğŸ“¥ Loaders â†’ âœ‚ï¸ Splitters â†’ ğŸ§  Gemini Embedder â†’ ğŸ’¾ Milvus Store
```

### ThÃ nh pháº§n thá»±c táº¿

| Module | Implementation | Há»— trá»£ |
|--------|----------------|---------|
| **Loaders** | LangChain document loaders | PyPDF, Docx2txt, TextLoader, WebBaseLoader |
| **Splitters** | RecursiveCharacterTextSplitter + Table splitter | Text (1000 chars) + Table (20 rows) |
| **Embedders** | Google Gemini text-embedding-004 | 768-dim vectors, retrieval_document/query |
| **Vector Store** | Milvus with COSINE metric | IVF_FLAT index, auto schema |
| **Pipeline** | RAGPipeline class | Automatic processing workflow |
| **Utils** | Text cleaner | Regex-based whitespace normalization |

## ğŸš€ CÃ i Ä‘áº·t vÃ  sá»­ dá»¥ng

### YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+ (khuyáº¿n nghá»‹ 3.9+)
- **Docker**: Äá»ƒ cháº¡y Milvus server
- **Memory**: Tá»‘i thiá»ƒu 4GB RAM (8GB+ khuyáº¿n nghá»‹)
- **Disk**: 2GB+ dung lÆ°á»£ng trá»‘ng

### BÆ°á»›c 1: CÃ i Ä‘áº·t dá»± Ã¡n

```bash
# Clone repository
git clone https://github.com/yourusername/file2rag.git
cd file2rag

# Táº¡o virtual environment (khuyáº¿n nghá»‹)
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### BÆ°á»›c 2: Láº¥y Gemini API Key

1. Truy cáº­p [Google AI Studio](https://aistudio.google.com/app/apikey)
2. ÄÄƒng nháº­p vá»›i Google account
3. Táº¡o API key má»›i
4. Copy API key Ä‘á»ƒ sá»­ dá»¥ng

### BÆ°á»›c 3: Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng

```bash
# Táº¡o file .env trong thÆ° má»¥c gá»‘c cá»§a project
echo "GEMINI_API_KEY=your_actual_api_key_here" > .env
echo "MILVUS_HOST=localhost" >> .env
echo "MILVUS_PORT=19530" >> .env

# Kiá»ƒm tra file .env
cat .env
```

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Milvus

Xem pháº§n **Khá»Ÿi Ä‘á»™ng Milvus (Docker)** bÃªn dÆ°á»›i Ä‘á»ƒ cÃ³ hÆ°á»›ng dáº«n chi tiáº¿t.

### BÆ°á»›c 5: Test installation

```python
# Test script - táº¡o file test_installation.py
import os
from dotenv import load_dotenv

print("ğŸ§ª Testing File2RAG installation...")

# Test 1: Environment variables
load_dotenv()
api_key = os.getenv('GEMINI_API_KEY')
print(f"âœ… Gemini API key loaded: {'Yes' if api_key else 'No'}")

# Test 2: Milvus connection
try:
    from pymilvus import connections
    connections.connect("default", host="localhost", port="19530")
    print("âœ… Milvus connection: Success")
    connections.disconnect("default")
except Exception as e:
    print(f"âŒ Milvus connection: Failed - {e}")

# Test 3: Gemini API
try:
    import google.generativeai as genai
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('models/text-embedding-004')
    print("âœ… Gemini API: Success")
except Exception as e:
    print(f"âŒ Gemini API: Failed - {e}")

# Test 4: RAG Pipeline
try:
    from rag.pipeline.rag_pipeline import RAGPipeline
    print("âœ… RAG Pipeline import: Success")
except Exception as e:
    print(f"âŒ RAG Pipeline import: Failed - {e}")

print("\nğŸ‰ Installation test completed!")
```

Cháº¡y test:
```bash
python test_installation.py
```

### Khá»Ÿi Ä‘á»™ng Milvus (Docker)

#### PhÆ°Æ¡ng phÃ¡p 1: Milvus Standalone (ÄÆ¡n giáº£n - Khuyáº¿n nghá»‹)

```bash
# BÆ°á»›c 1: Pull vÃ  cháº¡y Milvus standalone
docker run -d \
  --name milvus-standalone \
  --security-opt seccomp:unconfined \
  -p 19530:19530 \
  -p 9091:9091 \
  -v ${PWD}/volumes/milvus:/var/lib/milvus \
  milvusdb/milvus:v2.3.3

# BÆ°á»›c 2: Kiá»ƒm tra container Ä‘Ã£ cháº¡y
docker ps | findstr milvus

# BÆ°á»›c 3: Kiá»ƒm tra logs
docker logs milvus-standalone

# BÆ°á»›c 4: Test káº¿t ná»‘i
python -c "from pymilvus import connections; connections.connect('default', host='localhost', port='19530'); print('âœ… Milvus connected successfully!')"
```

#### PhÆ°Æ¡ng phÃ¡p 2: Docker Compose (Production)

Táº¡o file `docker-compose.yml`:

```yaml
version: '3.5'

services:
  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio:
    container_name: milvus-minio
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    ports:
      - "9001:9001"
      - "9000:9000"
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  milvus:
    container_name: milvus-standalone
    image: milvusdb/milvus:v2.3.3
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 3
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - "etcd"
      - "minio"
```

Cháº¡y vá»›i Docker Compose:
```bash
docker-compose up -d
```

#### Troubleshooting Milvus

```bash
# Kiá»ƒm tra cÃ¡c container Ä‘ang cháº¡y
docker ps -a

# Xem logs cá»§a Milvus
docker logs milvus-standalone -f

# Restart Milvus náº¿u cáº§n
docker restart milvus-standalone

# Dá»n dáº¹p vÃ  khá»Ÿi Ä‘á»™ng láº¡i
docker stop milvus-standalone
docker rm milvus-standalone
# Cháº¡y láº¡i lá»‡nh docker run á»Ÿ trÃªn

# Test káº¿t ná»‘i tá»« Python
python -c "
from pymilvus import connections, utility
try:
    connections.connect('default', host='localhost', port='19530')
    print('âœ… Káº¿t ná»‘i Milvus thÃ nh cÃ´ng!')
    print(f'Server version: {utility.get_server_version()}')
except Exception as e:
    print(f'âŒ Lá»—i káº¿t ná»‘i: {e}')
"
```

## ğŸ“‹ HÆ°á»›ng dáº«n sá»­ dá»¥ng thá»±c táº¿

### Chuáº©n bá»‹ environment

```bash
# 1. Táº¡o file .env trong thÆ° má»¥c dá»± Ã¡n
echo "GEMINI_API_KEY=your_gemini_api_key_here" > .env
echo "MILVUS_HOST=localhost" >> .env
echo "MILVUS_PORT=19530" >> .env

# 2. CÃ i Ä‘áº·t Python dependencies
pip install -r requirements.txt

# 3. Khá»Ÿi Ä‘á»™ng Milvus (xem pháº§n trÃªn)
```

### VÃ­ dá»¥ 1: Xá»­ lÃ½ file PDF Ä‘Æ¡n giáº£n

```python
import os
from dotenv import load_dotenv
from rag.pipeline.rag_pipeline import RAGPipeline

# Load environment variables
load_dotenv()

# Khá»Ÿi táº¡o pipeline vá»›i collection name
rag = RAGPipeline(collection_name="my_research_papers")

# Xá»­ lÃ½ file PDF
document_id = rag.process_document("./data/machine_learning_paper.pdf")
print(f"âœ… Document processed with ID: {document_id}")

# Output máº«u:
# ğŸš€ Initializing RAG Pipeline...
# âœ… RAG Pipeline initialized successfully!
# 
# ğŸ“„ Processing document: ./data/machine_learning_paper.pdf
# ğŸ“‚ Loading document...
#    Loaded 15 document sections  
# âœ‚ï¸ Chunking documents...
#    Created 47 chunks
# ğŸ§  Creating embeddings...
#    Generated 47 embeddings
# ğŸ’¾ Storing in vector database...
# âœ… Document processed successfully in 12.34s
#    Document ID: doc_1703123456789
```

### VÃ­ dá»¥ 2: Xá»­ lÃ½ nhiá»u loáº¡i file

```python
from rag.pipeline.rag_pipeline import RAGPipeline

# Khá»Ÿi táº¡o pipeline cho collection khÃ¡c nhau
knowledge_base = RAGPipeline(collection_name="company_knowledge")

# Xá»­ lÃ½ cÃ¡c loáº¡i file khÃ¡c nhau
files_to_process = [
    "./documents/company_handbook.pdf",      # PDF document
    "./documents/employee_data.csv",         # CSV table  
    "./documents/quarterly_report.docx",     # Word document
    "./documents/meeting_notes.txt",         # Text file
    "./documents/budget_2024.xlsx",          # Excel spreadsheet
    "https://company.com/blog/new-policy"    # Web URL
]

processed_docs = []
for file_path in files_to_process:
    try:
        doc_id = knowledge_base.process_document(file_path)
        processed_docs.append({
            'file': file_path,
            'document_id': doc_id,
            'status': 'success'
        })
        print(f"âœ… Processed: {file_path}")
    except Exception as e:
        processed_docs.append({
            'file': file_path,
            'document_id': None,
            'status': 'failed',
            'error': str(e)
        })
        print(f"âŒ Failed: {file_path} - {e}")

# In káº¿t quáº£
for doc in processed_docs:
    print(f"File: {doc['file']}")
    print(f"Status: {doc['status']}")
    if doc['status'] == 'success':
        print(f"Document ID: {doc['document_id']}")
    else:
        print(f"Error: {doc['error']}")
    print("-" * 50)
```

### VÃ­ dá»¥ 3: Xá»­ lÃ½ vÃ  kiá»ƒm tra dá»¯ liá»‡u trong Milvus

```python
from rag.pipeline.rag_pipeline import RAGPipeline
from pymilvus import connections, Collection

# Khá»Ÿi táº¡o pipeline
rag = RAGPipeline(collection_name="document_store")

# Xá»­ lÃ½ document
doc_id = rag.process_document("./sample.pdf")

# Káº¿t ná»‘i trá»±c tiáº¿p vá»›i Milvus Ä‘á»ƒ kiá»ƒm tra dá»¯ liá»‡u
connections.connect("default", host="localhost", port="19530")
collection = Collection("document_store")

# Kiá»ƒm tra thÃ´ng tin collection
print(f"Collection name: {collection.name}")
print(f"Collection entities: {collection.num_entities}")
print(f"Collection schema: {collection.schema}")

# Load collection Ä‘á»ƒ cÃ³ thá»ƒ query
collection.load()

# TÃ¬m kiáº¿m vector tÆ°Æ¡ng tá»± (demo - cáº§n embedding cá»§a query)
# search_results = collection.search(
#     data=[query_embedding],  # Cáº§n táº¡o embedding cho query
#     anns_field="embedding",
#     param={"metric_type": "COSINE", "params": {"nprobe": 10}},
#     limit=5,
#     output_fields=["content", "source", "page"]
# )
```

### 3. Chunking strategies Ä‘Æ°á»£c Ã¡p dá»¥ng tá»± Ä‘á»™ng

```python
# Text files (.pdf, .docx, .txt, URLs): chunk_text_medium
# - Chunk size: 1000 characters
# - Overlap: 200 characters  
# - Splitter: RecursiveCharacterTextSplitter

# Table files (.csv, .xlsx): chunk_documents_by_rows  
# - Chunk size: 20 rows per chunk
# - Split by double newlines (\n\n)
```

### 4. Vector store operations

```python
# Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trong Milvus vá»›i schema:
# - id: INT64 (auto-generated primary key)
# - embedding: FLOAT_VECTOR (768 dimensions)
# - content: VARCHAR(65535) 
# - source: VARCHAR(1000)
# - page: INT64
# - content_type: VARCHAR(100)
# - chunk_index: INT64

# Index: IVF_FLAT vá»›i COSINE similarity
```

## ğŸ”§ Cáº¥u hÃ¬nh vÃ  customization

### Text Splitter Settings

```python
# File: rag/splitters/text_splitter.py
CHUNK_SIZE = 1000  # Characters per chunk
CHUNK_OVERLAP = 200  # Overlap between chunks

# Presets cÃ³ sáºµn:
chunk_text_small(documents)   # 500 chars, 100 overlap
chunk_text_medium(documents)  # 1000 chars, 200 overlap  
chunk_text_large(documents)   # 2000 chars, 400 overlap
```

### Table Splitter Settings

```python
# File: rag/splitters/table_splitter.py  
CHUNK_SIZE = 20  # Rows per chunk

# Presets cÃ³ sáºµn:
chunk_table_small(documents)   # 10 rows per chunk
chunk_table_medium(documents)  # 20 rows per chunk
chunk_table_large(documents)   # 50 rows per chunk
```

### Gemini Embedder Configuration

```python
# File: rag/embedders/gemini_embedder.py
model_name = "models/text-embedding-004"
dimension = 768
task_type = "retrieval_document" hoáº·c "retrieval_query"
```

### Milvus Vector Store Schema

```python
# Auto-created schema trong MilvusVectorStore:
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=1000),
    FieldSchema(name="page", dtype=DataType.INT64),
    FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="chunk_index", dtype=DataType.INT64)
]
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n thá»±c táº¿

```
file2rag/
â”œâ”€â”€ ğŸ“ rag/                           # Core RAG modules
â”‚   â”œâ”€â”€ ğŸ“ embedders/                 # AI embedding providers  
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ gemini_embedder.py        # Google Gemini text-embedding-004
â”‚   â”œâ”€â”€ ğŸ“ loaders/                   # Document loaders
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py                # DocumentLoaderManager class
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py             # PyPDFLoader wrapper
â”‚   â”‚   â”œâ”€â”€ docx_loader.py            # Docx2txtLoader wrapper
â”‚   â”‚   â”œâ”€â”€ text_loader.py            # TextLoader wrapper
â”‚   â”‚   â”œâ”€â”€ csv_loader.py             # Pandas-based CSV loader
â”‚   â”‚   â”œâ”€â”€ xlsx_loader.py            # Pandas-based Excel loader
â”‚   â”‚   â””â”€â”€ url_loader.py             # WebBaseLoader wrapper
â”‚   â”œâ”€â”€ ğŸ“ splitters/                 # Text chunking strategies
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_splitter.py          # RecursiveCharacterTextSplitter
â”‚   â”‚   â””â”€â”€ table_splitter.py         # Row-based table splitting
â”‚   â”œâ”€â”€ ğŸ“ vector_stores/             # Vector database interfaces
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ milvus_vector_store.py    # Milvus client wrapper
â”‚   â””â”€â”€ ğŸ“ pipeline/                  # Main pipeline
â”‚       â””â”€â”€ rag_pipeline.py           # RAGPipeline class
â”œâ”€â”€ ğŸ“ utils/                         # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ text_cleaner.py               # Regex-based text cleaning
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â””â”€â”€ ğŸ“„ README.md                     # TÃ i liá»‡u nÃ y
```

### Key Dependencies

```pip-requirements
# Core framework
langchain-community     # Document loaders
langchain-core         # Document classes
langchain             # Text splitters

# Document processing
docx2txt              # DOCX extraction
pypdf                 # PDF processing  
pandas                # CSV/Excel handling
openpyxl              # Excel support
beautifulsoup4        # Web scraping
requests              # HTTP requests

# AI & Vector DB
google-generativeai   # Gemini embeddings
pymilvus              # Milvus client

# Environment
python-dotenv         # .env file support
```


## ğŸ“Š Hiá»‡u suáº¥t vÃ  giá»›i háº¡n

### Processing Performance

| Operation | Typical Time | Notes |
|-----------|-------------|-------|
| PDF Loading (10 pages) | ~2-5s | Depends on PDF complexity |
| Text Chunking | ~100-500ms | For 1000 chars/chunk |
| Gemini Embedding | ~300-800ms per request | Rate limited by API |
| Milvus Storage | ~50-200ms | Per batch of chunks |
| Vector Search | <100ms | For top-k retrieval |

### Current Limitations

- **No search interface**: Chá»‰ cÃ³ process_document, chÆ°a cÃ³ search method
- **No batch embedding**: Embed tá»«ng chunk riÃªng láº» 
- **Limited error handling**: Basic exception catching
- **No persistence**: KhÃ´ng save/load pipeline state
- **Memory usage**: KhÃ´ng tá»‘i Æ°u cho large documents
- **API rate limits**: Gemini API cÃ³ giá»›i háº¡n requests/minute

### Chunking Behavior

```python
# Text files (.pdf, .docx, .txt, URLs):
# - RecursiveCharacterTextSplitter
# - Default: 1000 chars, 200 overlap
# - Separators: ["\n\n", "\n", " ", ""]

# Table files (.csv, .xlsx):  
# - Split by rows (double newlines)
# - Default: 20 rows per chunk
# - Format: key:value pairs per row
```

### ğŸš¦ Lá»—i thÆ°á»ng gáº·p vÃ  cÃ¡ch kháº¯c phá»¥c

#### 1. Lá»—i káº¿t ná»‘i Milvus

**Lá»—i:** `pymilvus.exceptions.MilvusException: <MilvusException: (code=1, message=Fail connecting to server on localhost:19530...)`

**NguyÃªn nhÃ¢n & Kháº¯c phá»¥c:**
```bash
# Kiá»ƒm tra Milvus container
docker ps | findstr milvus

# Náº¿u khÃ´ng cÃ³ container nÃ o, khá»Ÿi Ä‘á»™ng Milvus
docker run -d --name milvus-standalone -p 19530:19530 -p 9091:9091 milvusdb/milvus:v2.3.3

# Náº¿u container Ä‘Ã£ tá»“n táº¡i nhÆ°ng stopped
docker start milvus-standalone

# Kiá»ƒm tra logs Ä‘á»ƒ debug
docker logs milvus-standalone --tail 50

# Test káº¿t ná»‘i
python -c "from pymilvus import connections; connections.connect('default', host='localhost', port='19530'); print('âœ… Connected!')"
```

#### 2. Lá»—i Gemini API

**Lá»—i:** `google.api_core.exceptions.Unauthorized: 401 API_KEY_INVALID`

**NguyÃªn nhÃ¢n & Kháº¯c phá»¥c:**
```bash
# Kiá»ƒm tra API key trong .env file
cat .env | findstr GEMINI_API_KEY

# Táº¡o API key má»›i táº¡i: https://aistudio.google.com/app/apikey
# ThÃªm vÃ o .env file:
echo "GEMINI_API_KEY=your_actual_api_key_here" > .env

# Test API key
python -c "
import google.generativeai as genai
import os
from dotenv import load_dotenv
load_dotenv()
genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
print('âœ… Gemini API key valid!')
"
```

#### 3. Lá»—i import modules

**Lá»—i:** `ModuleNotFoundError: No module named 'rag.loaders'`

**NguyÃªn nhÃ¢n & Kháº¯c phá»¥c:**
```python
# Trong rag_pipeline.py, Ä‘áº£m báº£o cÃ³:
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Hoáº·c cháº¡y tá»« thÆ° má»¥c gá»‘c cá»§a project:
cd /path/to/file2rag
python -c "from rag.pipeline.rag_pipeline import RAGPipeline; print('âœ… Import successful!')"
```

#### 4. Lá»—i xá»­ lÃ½ file

**Lá»—i:** `FileNotFoundError: Document not found: document.pdf`

**Kháº¯c phá»¥c:**
```python
import os

file_path = "document.pdf"
print(f"File exists: {os.path.exists(file_path)}")
print(f"Absolute path: {os.path.abspath(file_path)}")

# Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
full_path = os.path.abspath(file_path)
doc_id = rag.process_document(full_path)
```

#### 5. Lá»—i memory vá»›i file lá»›n

**Lá»—i:** `MemoryError` hoáº·c quÃ¡ trÃ¬nh xá»­ lÃ½ quÃ¡ cháº­m

**Kháº¯c phá»¥c:**
```python
# Äiá»u chá»‰nh chunk size nhá» hÆ¡n
from rag.splitters.text_splitter import chunk_text_small

# Hoáº·c tÃ¹y chá»‰nh chunk size
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,      # Giáº£m tá»« 1000
    chunk_overlap=50,    # Giáº£m tá»« 200
    length_function=len,
)
```

#### 6. Lá»—i encoding vá»›i file text

**Lá»—i:** `UnicodeDecodeError: 'utf-8' codec can't decode byte...`

**Kháº¯c phá»¥c:**
```python
# Kiá»ƒm tra encoding cá»§a file
import chardet

with open("file.txt", "rb") as f:
    encoding = chardet.detect(f.read())
    print(f"Detected encoding: {encoding}")

# Convert sang UTF-8 náº¿u cáº§n
with open("file.txt", "r", encoding="latin-1") as f:
    content = f.read()
    
with open("file_utf8.txt", "w", encoding="utf-8") as f:
    f.write(content)
```

#### 7. Performance issues

**Váº¥n Ä‘á»:** Xá»­ lÃ½ file quÃ¡ cháº­m

**Tá»‘i Æ°u hÃ³a:**
```python
# 1. Giáº£m chunk size
# 2. Batch embedding (cáº§n implement)
# 3. Parallel processing (future feature)

# Monitor performance
import time
start = time.time()
doc_id = rag.process_document("large_file.pdf")
print(f"Processing time: {time.time() - start:.2f}s")

# Æ¯á»›c tÃ­nh thá»i gian cho file lá»›n:
# - 1MB PDF: ~5-10 giÃ¢y
# - 10MB PDF: ~30-60 giÃ¢y  
# - 100MB file: cÃ³ thá»ƒ máº¥t vÃ i phÃºt
``` 
## ğŸ“ Development Notes

### Current Implementation Status

âœ… **HoÃ n thÃ nh:**
- Document loaders cho 6 Ä‘á»‹nh dáº¡ng (PDF, DOCX, TXT, CSV, XLSX, URL)
- Text vÃ  table splitters vá»›i presets
- Gemini embedder integration
- Milvus vector store vá»›i auto schema
- End-to-end pipeline workflow
- Basic text cleaning utilities
- Test framework cho pipeline

ğŸ”„ **Cáº§n hoÃ n thiá»‡n:**
- Search/retrieval interface (chá»‰ cÃ³ embedding/storage)
- Batch processing optimization  
- Advanced filtering vÃ  metadata queries
- Performance monitoring
- Error recovery mechanisms
- Documentation examples vá»›i real data

### Code Structure Notes

- **Loaders**: Wrapper functions around LangChain loaders
- **Splitters**: Preset configurations, easily customizable
- **Pipeline**: Single `process_document()` method handles full workflow
- **Vector Store**: Auto-creates schema, COSINE similarity default
- **Utils**: Minimal text preprocessing (whitespace cleaning)

### Next Development Steps

1. Implement search/retrieval methods
2. Add batch embedding support  
3. Improve error handling vÃ  retry logic
4. Add performance metrics vÃ  monitoring
5. Create web interface hoáº·c API endpoints
6. Add support for more embedding models

## ğŸ“„ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) for details.
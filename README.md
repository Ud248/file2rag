# ğŸš€ File2RAG

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) Ä‘Æ¡n giáº£n vÃ  hiá»‡u quáº£, giÃºp chuyá»ƒn Ä‘á»•i tÃ i liá»‡u thÃ nh cÆ¡ sá»Ÿ tri thá»©c cÃ³ thá»ƒ tÃ¬m kiáº¿m báº±ng cÃ´ng nghá»‡ vector embedding vÃ  Milvus vector database.

## âœ¨ TÃ­nh nÄƒng chÃ­nh

- **ğŸ“„ Há»— trá»£ Ä‘a Ä‘á»‹nh dáº¡ng**: PDF, DOCX, TXT, CSV, XLSX vÃ  URL web
- **ğŸ§  Text Chunking thÃ´ng minh**: Tá»± Ä‘á»™ng chá»n chiáº¿n lÆ°á»£c phÃ¹ há»£p cho tá»«ng loáº¡i tÃ i liá»‡u
- **ğŸ” TÃ¬m kiáº¿m vector**: Powered by Google Gemini text-embedding-004 (768 dimensions)
- **ğŸ’¾ Milvus Vector Database**: LÆ°u trá»¯ vÃ  tÃ¬m kiáº¿m COSINE similarity hiá»‡u suáº¥t cao
- **âš¡ Pipeline tá»± Ä‘á»™ng**: Load â†’ Chunk â†’ Embed â†’ Store trong má»™t lá»‡nh
- **ğŸ”§ Chunking linh hoáº¡t**: Cáº¥u hÃ¬nh chunk size cho text vÃ  table data
- **ğŸ§¹ Text preprocessing**: Tá»± Ä‘á»™ng lÃ m sáº¡ch excessive whitespace

## ğŸ—ï¸ Kiáº¿n trÃºc Ä‘Æ¡n giáº£n

```
ğŸ“„ Documents â†’ ğŸ“¥ Loaders â†’ âœ‚ï¸ Splitters â†’ ğŸ§  Gemini Embedder â†’ ğŸ’¾ Milvus Store
```

### ThÃ nh pháº§n thá»±c táº¿

| Module | Implementation | Há»— trá»£ |
|--------|----------------|---------|
| **Loaders** | LangChain document loaders | PyPDF, Docx2txt, TextLoader, WebBaseLoader |
| **Splitters** | RecursiveCharacterTextSplitter + Table splitter | Text (1000 chars) + Table (20 rows) |
| **Embedders** | Google Gemini text-embedding-004 | 768-dim vectors, retrieval_document/query |
| **Vector Store** | Milvus with COSINE metric | IVF_FLAT index, auto schema |
| **Pipeline** | RAGPipeline class | Automatic processing workflow |
| **Utils** | Text cleaner | Regex-based whitespace normalization |

## ğŸš€ CÃ i Ä‘áº·t vÃ  sá»­ dá»¥ng

### YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.8+
- **Milvus**: Server instance (local/cloud)
- **API Key**: Google AI Studio (cho Gemini embeddings)

### CÃ i Ä‘áº·t nhanh

```bash
# 1. Clone repository
git clone https://github.com/yourusername/file2rag.git
cd file2rag

# 2. CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# 3. Thiáº¿t láº­p mÃ´i trÆ°á»ng
# Táº¡o file .env vÃ  thÃªm API key
GEMINI_API_KEY=your_gemini_api_key_here
MILVUS_HOST=localhost
MILVUS_PORT=19530
```

### Khá»Ÿi Ä‘á»™ng Milvus (Docker)

```bash
# Download vÃ  cháº¡y Milvus standalone
docker run -d --name milvus-standalone \
  -p 19530:19530 \
  -v milvus_data:/var/lib/milvus \
  milvusdb/milvus:latest

# Kiá»ƒm tra status
docker ps | grep milvus
```

## ğŸ“‹ HÆ°á»›ng dáº«n sá»­ dá»¥ng thá»±c táº¿

### 1. Sá»­ dá»¥ng cÆ¡ báº£n

```python
from rag.pipeline.rag_pipeline import RAGPipeline

# Khá»Ÿi táº¡o pipeline
rag = RAGPipeline(collection_name="my_documents")

# Xá»­ lÃ½ tÃ i liá»‡u (tá»± Ä‘á»™ng: load â†’ chunk â†’ embed â†’ store)
document_id = rag.process_document("path/to/document.pdf")
print(f"âœ… Processed document: {document_id}")

# TÃ¬m kiáº¿m (cáº§n implement thÃªm search method)
# results = rag.search("Machine learning trong healthcare", k=5)
```

### 2. Xá»­ lÃ½ cÃ¡c loáº¡i file khÃ¡c nhau

```python
# PDF documents
doc_id = rag.process_document("research_paper.pdf")

# Word documents  
doc_id = rag.process_document("report.docx")

# Text files
doc_id = rag.process_document("notes.txt")

# CSV files (sáº½ dÃ¹ng table splitter)
doc_id = rag.process_document("data.csv")

# Excel files (sáº½ dÃ¹ng table splitter)  
doc_id = rag.process_document("spreadsheet.xlsx")

# Web URLs
doc_id = rag.process_document("https://example.com/article")
```

### 3. Chunking strategies Ä‘Æ°á»£c Ã¡p dá»¥ng tá»± Ä‘á»™ng

```python
# Text files (.pdf, .docx, .txt, URLs): chunk_text_medium
# - Chunk size: 1000 characters
# - Overlap: 200 characters  
# - Splitter: RecursiveCharacterTextSplitter

# Table files (.csv, .xlsx): chunk_documents_by_rows  
# - Chunk size: 20 rows per chunk
# - Split by double newlines (\n\n)
```

### 4. Vector store operations

```python
# Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trong Milvus vá»›i schema:
# - id: INT64 (auto-generated primary key)
# - embedding: FLOAT_VECTOR (768 dimensions)
# - content: VARCHAR(65535) 
# - source: VARCHAR(1000)
# - page: INT64
# - content_type: VARCHAR(100)
# - chunk_index: INT64

# Index: IVF_FLAT vá»›i COSINE similarity
```

## ğŸ”§ Cáº¥u hÃ¬nh vÃ  customization

### Text Splitter Settings

```python
# File: rag/splitters/text_splitter.py
CHUNK_SIZE = 1000  # Characters per chunk
CHUNK_OVERLAP = 200  # Overlap between chunks

# Presets cÃ³ sáºµn:
chunk_text_small(documents)   # 500 chars, 100 overlap
chunk_text_medium(documents)  # 1000 chars, 200 overlap  
chunk_text_large(documents)   # 2000 chars, 400 overlap
```

### Table Splitter Settings

```python
# File: rag/splitters/table_splitter.py  
CHUNK_SIZE = 20  # Rows per chunk

# Presets cÃ³ sáºµn:
chunk_table_small(documents)   # 10 rows per chunk
chunk_table_medium(documents)  # 20 rows per chunk
chunk_table_large(documents)   # 50 rows per chunk
```

### Gemini Embedder Configuration

```python
# File: rag/embedders/gemini_embedder.py
model_name = "models/text-embedding-004"
dimension = 768
task_type = "retrieval_document" hoáº·c "retrieval_query"
```

### Milvus Vector Store Schema

```python
# Auto-created schema trong MilvusVectorStore:
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=1000),
    FieldSchema(name="page", dtype=DataType.INT64),
    FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="chunk_index", dtype=DataType.INT64)
]
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n thá»±c táº¿

```
file2rag/
â”œâ”€â”€ ğŸ“ rag/                           # Core RAG modules
â”‚   â”œâ”€â”€ ğŸ“ embedders/                 # AI embedding providers  
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ gemini_embedder.py        # Google Gemini text-embedding-004
â”‚   â”œâ”€â”€ ğŸ“ loaders/                   # Document loaders
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py                # DocumentLoaderManager class
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py             # PyPDFLoader wrapper
â”‚   â”‚   â”œâ”€â”€ docx_loader.py            # Docx2txtLoader wrapper
â”‚   â”‚   â”œâ”€â”€ text_loader.py            # TextLoader wrapper
â”‚   â”‚   â”œâ”€â”€ csv_loader.py             # Pandas-based CSV loader
â”‚   â”‚   â”œâ”€â”€ xlsx_loader.py            # Pandas-based Excel loader
â”‚   â”‚   â””â”€â”€ url_loader.py             # WebBaseLoader wrapper
â”‚   â”œâ”€â”€ ğŸ“ splitters/                 # Text chunking strategies
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_splitter.py          # RecursiveCharacterTextSplitter
â”‚   â”‚   â””â”€â”€ table_splitter.py         # Row-based table splitting
â”‚   â”œâ”€â”€ ğŸ“ vector_stores/             # Vector database interfaces
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ milvus_vector_store.py    # Milvus client wrapper
â”‚   â””â”€â”€ ğŸ“ pipeline/                  # Main pipeline
â”‚       â””â”€â”€ rag_pipeline.py           # RAGPipeline class
â”œâ”€â”€ ğŸ“ utils/                         # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ text_cleaner.py               # Regex-based text cleaning
â”œâ”€â”€ ğŸ“ test/                          # Test suite
â”‚   â”œâ”€â”€ test_chunkers.py
â”‚   â”œâ”€â”€ test_embedding.py
â”‚   â”œâ”€â”€ test_loaders.py
â”‚   â””â”€â”€ test_pipeline.py              # Pipeline testing
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â””â”€â”€ ğŸ“„ README.md                     # TÃ i liá»‡u nÃ y
```

### Key Dependencies

```pip-requirements
# Core framework
langchain-community     # Document loaders
langchain-core         # Document classes
langchain             # Text splitters

# Document processing
docx2txt              # DOCX extraction
pypdf                 # PDF processing  
pandas                # CSV/Excel handling
openpyxl              # Excel support
beautifulsoup4        # Web scraping
requests              # HTTP requests

# AI & Vector DB
google-generativeai   # Gemini embeddings
pymilvus              # Milvus client

# Environment
python-dotenv         # .env file support
```

## ğŸ§ª Testing

Cháº¡y tests cÃ³ sáºµn:

```bash
# Test process_document vá»›i file cá»¥ thá»ƒ
python test\test_pipeline.py "path/to/file.pdf"

# Test vá»›i URL
python test\test_pipeline.py "https://example.com"

# Test vá»›i document ID custom
python test\test_pipeline.py "file.txt" --doc-id "custom_123" 

# Test multiple file types
python test\test_pipeline.py --multi

# Test error handling
python test\test_pipeline.py --errors

# Cháº¡y táº¥t cáº£ tests
python test\test_pipeline.py --all
```

Test output example:
```
ğŸ§ª TESTING PROCESS_DOCUMENT FUNCTION ONLY
File: sample.pdf
Document ID: Auto-generated
======================================================================

ğŸš€ Initializing RAG Pipeline...
âœ… RAG Pipeline initialized successfully!

ğŸ“„ Calling process_document('sample.pdf', 'None')...
--------------------------------------------------
ğŸ“‚ Loading document...
   Loaded 1 document sections
âœ‚ï¸ Chunking documents...
   Created 5 chunks
ğŸ§  Creating embeddings...
   Generated 5 embeddings
ğŸ’¾ Storing in vector database...
Added 5 documents to test_process_doc
âœ… Document processed successfully in 3.45s
   Document ID: 1234567890
```

## ğŸ“Š Hiá»‡u suáº¥t vÃ  giá»›i háº¡n

### Processing Performance

| Operation | Typical Time | Notes |
|-----------|-------------|-------|
| PDF Loading (10 pages) | ~2-5s | Depends on PDF complexity |
| Text Chunking | ~100-500ms | For 1000 chars/chunk |
| Gemini Embedding | ~300-800ms per request | Rate limited by API |
| Milvus Storage | ~50-200ms | Per batch of chunks |
| Vector Search | <100ms | For top-k retrieval |

### Current Limitations

- **No search interface**: Chá»‰ cÃ³ process_document, chÆ°a cÃ³ search method
- **No batch embedding**: Embed tá»«ng chunk riÃªng láº» 
- **Limited error handling**: Basic exception catching
- **No persistence**: KhÃ´ng save/load pipeline state
- **Memory usage**: KhÃ´ng tá»‘i Æ°u cho large documents
- **API rate limits**: Gemini API cÃ³ giá»›i háº¡n requests/minute

### Chunking Behavior

```python
# Text files (.pdf, .docx, .txt, URLs):
# - RecursiveCharacterTextSplitter
# - Default: 1000 chars, 200 overlap
# - Separators: ["\n\n", "\n", " ", ""]

# Table files (.csv, .xlsx):  
# - Split by rows (double newlines)
# - Default: 20 rows per chunk
# - Format: key:value pairs per row
```

## ğŸš¦ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p

**1. Milvus Connection Error**
```bash
# Kiá»ƒm tra Milvus container
docker ps | grep milvus
docker logs <milvus_container_id>

# Khá»Ÿi Ä‘á»™ng láº¡i Milvus
docker run -d --name milvus-standalone -p 19530:19530 milvusdb/milvus:latest
```

**2. Gemini API Error**  
```python
# Kiá»ƒm tra API key trong .env
import os
from dotenv import load_dotenv
load_dotenv()
print(f"API Key exists: {bool(os.getenv('GEMINI_API_KEY'))}")
```

**3. File Loading Error**
```python
# Check file path vÃ  supported extensions
supported = ['.pdf', '.docx', '.txt', '.csv', '.xlsx']
file_ext = os.path.splitext(file_path)[1].lower()
print(f"Extension {file_ext} supported: {file_ext in supported}")
```

**4. Import Error**
```python
# Check sys.path trong rag_pipeline.py
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
```

### Debug Steps

1. **Test tá»«ng component**:
```bash
python -c "from rag.embedders.gemini_embedder import GeminiEmbedder; e=GeminiEmbedder(); print('Embedder OK')"
python -c "from rag.vector_stores.milvus_vector_store import MilvusVectorStore; v=MilvusVectorStore(); print('Milvus OK')"
```

2. **Check dependencies**:
```bash
pip list | grep -E "(langchain|pymilvus|google-generativeai)"
```

3. **Verbose logging**:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ Development Notes

### Current Implementation Status

âœ… **HoÃ n thÃ nh:**
- Document loaders cho 6 Ä‘á»‹nh dáº¡ng (PDF, DOCX, TXT, CSV, XLSX, URL)
- Text vÃ  table splitters vá»›i presets
- Gemini embedder integration
- Milvus vector store vá»›i auto schema
- End-to-end pipeline workflow
- Basic text cleaning utilities
- Test framework cho pipeline

ğŸ”„ **Cáº§n hoÃ n thiá»‡n:**
- Search/retrieval interface (chá»‰ cÃ³ embedding/storage)
- Batch processing optimization  
- Advanced filtering vÃ  metadata queries
- Performance monitoring
- Error recovery mechanisms
- Documentation examples vá»›i real data

### Code Structure Notes

- **Loaders**: Wrapper functions around LangChain loaders
- **Splitters**: Preset configurations, easily customizable
- **Pipeline**: Single `process_document()` method handles full workflow
- **Vector Store**: Auto-creates schema, COSINE similarity default
- **Utils**: Minimal text preprocessing (whitespace cleaning)

### Next Development Steps

1. Implement search/retrieval methods
2. Add batch embedding support  
3. Improve error handling vÃ  retry logic
4. Add performance metrics vÃ  monitoring
5. Create web interface hoáº·c API endpoints
6. Add support for more embedding models

## ğŸ“„ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- **Google AI Studio** - Gemini embeddings API
- **Milvus Team** - Vector database excellence
- **LangChain Community** - RAG inspiration
- **Vietnamese AI Community** - Support and feedback

## ğŸ“ Support & Contact

- ğŸ› **Issues**: [GitHub Issues](https://github.com/yourusername/file2rag/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/yourusername/file2rag/discussions)
- ğŸ“§ **Email**: support@file2rag.com
- ğŸŒ **Website**: [file2rag.com](https://file2rag.com)

---

<div align="center">

**ğŸ‡»ğŸ‡³ Made with â¤ï¸ for Vietnamese AI Community**

[â­ Star this repo](https://github.com/yourusername/file2rag) â€¢ [ğŸ› Report Bug](https://github.com/yourusername/file2rag/issues)